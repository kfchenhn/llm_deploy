2024-06-18 15:38:54 | INFO | model_worker | args: Namespace(host='localhost', port=41002, worker_address='http://localhost:41002', controller_address='http://localhost:41001', model_path='/home/c205/workspace/models/HuatuoGPT2-13B/', revision='main', device='cuda', gpus=None, num_gpus=1, max_gpu_memory=None, dtype=None, load_8bit=False, cpu_offloading=False, gptq_ckpt=None, gptq_wbits=16, gptq_groupsize=-1, gptq_act_order=False, awq_ckpt=None, awq_wbits=16, awq_groupsize=-1, enable_exllama=False, exllama_max_seq_len=4096, exllama_gpu_split=None, exllama_cache_8bit=False, enable_xft=False, xft_max_seq_len=4096, xft_dtype=None, model_names=None, conv_template=None, embed_in_truncate=False, limit_worker_concurrency=5, stream_interval=2, no_register=False, seed=None, debug=False, ssl=False)
2024-06-18 15:38:54 | INFO | model_worker | Loading the model ['HuatuoGPT2-13B'] on worker 256e7562 ...
2024-06-18 15:38:57 | ERROR | stderr | Traceback (most recent call last):
2024-06-18 15:38:57 | ERROR | stderr |   File "<frozen runpy>", line 198, in _run_module_as_main
2024-06-18 15:38:57 | ERROR | stderr |   File "<frozen runpy>", line 88, in _run_code
2024-06-18 15:38:57 | ERROR | stderr |   File "/home/c205/workspace/FastChat-main/fastchat/serve/model_worker.py", line 414, in <module>
2024-06-18 15:38:57 | ERROR | stderr |     args, worker = create_model_worker()
2024-06-18 15:38:57 | ERROR | stderr |                    ^^^^^^^^^^^^^^^^^^^^^
2024-06-18 15:38:57 | ERROR | stderr |   File "/home/c205/workspace/FastChat-main/fastchat/serve/model_worker.py", line 385, in create_model_worker
2024-06-18 15:38:57 | ERROR | stderr |     worker = ModelWorker(
2024-06-18 15:38:57 | ERROR | stderr |              ^^^^^^^^^^^^
2024-06-18 15:38:57 | ERROR | stderr |   File "/home/c205/workspace/FastChat-main/fastchat/serve/model_worker.py", line 77, in __init__
2024-06-18 15:38:57 | ERROR | stderr |     self.model, self.tokenizer = load_model(
2024-06-18 15:38:57 | ERROR | stderr |                                  ^^^^^^^^^^^
2024-06-18 15:38:57 | ERROR | stderr |   File "/home/c205/workspace/FastChat-main/fastchat/model/model_adapter.py", line 376, in load_model
2024-06-18 15:38:57 | ERROR | stderr |     model.to(device)
2024-06-18 15:38:57 | ERROR | stderr |   File "/home/c205/anaconda3/envs/fastchat-for-huatuo/lib/python3.11/site-packages/transformers/modeling_utils.py", line 2724, in to
2024-06-18 15:38:57 | ERROR | stderr |     return super().to(*args, **kwargs)
2024-06-18 15:38:57 | ERROR | stderr |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-06-18 15:38:57 | ERROR | stderr |   File "/home/c205/anaconda3/envs/fastchat-for-huatuo/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1173, in to
2024-06-18 15:38:57 | ERROR | stderr |     return self._apply(convert)
2024-06-18 15:38:57 | ERROR | stderr |            ^^^^^^^^^^^^^^^^^^^^
2024-06-18 15:38:57 | ERROR | stderr |   File "/home/c205/anaconda3/envs/fastchat-for-huatuo/lib/python3.11/site-packages/torch/nn/modules/module.py", line 779, in _apply
2024-06-18 15:38:57 | ERROR | stderr |     module._apply(fn)
2024-06-18 15:38:57 | ERROR | stderr |   File "/home/c205/anaconda3/envs/fastchat-for-huatuo/lib/python3.11/site-packages/torch/nn/modules/module.py", line 779, in _apply
2024-06-18 15:38:57 | ERROR | stderr |     module._apply(fn)
2024-06-18 15:38:57 | ERROR | stderr |   File "/home/c205/anaconda3/envs/fastchat-for-huatuo/lib/python3.11/site-packages/torch/nn/modules/module.py", line 779, in _apply
2024-06-18 15:38:57 | ERROR | stderr |     module._apply(fn)
2024-06-18 15:38:57 | ERROR | stderr |   [Previous line repeated 2 more times]
2024-06-18 15:38:57 | ERROR | stderr |   File "/home/c205/anaconda3/envs/fastchat-for-huatuo/lib/python3.11/site-packages/torch/nn/modules/module.py", line 804, in _apply
2024-06-18 15:38:57 | ERROR | stderr |     param_applied = fn(param)
2024-06-18 15:38:57 | ERROR | stderr |                     ^^^^^^^^^
2024-06-18 15:38:57 | ERROR | stderr |   File "/home/c205/anaconda3/envs/fastchat-for-huatuo/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1159, in convert
2024-06-18 15:38:57 | ERROR | stderr |     return t.to(
2024-06-18 15:38:57 | ERROR | stderr |            ^^^^^
2024-06-18 15:38:57 | ERROR | stderr | torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 134.00 MiB. GPU
